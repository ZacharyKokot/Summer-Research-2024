\documentclass[12pt]{article}

\usepackage[letterpaper]{geometry} % For setting the margins

\geometry{
    left=30mm,
    right=30mm,
    top=30mm,
    bottom=40mm,
    headheight = 15pt
}

\usepackage{fancyhdr} % For setting the header and footer
 % Creates a fancy header.
 \pagestyle{fancy}
 \fancyhf{}
 \rhead{\thepage}
 \renewcommand{\headrulewidth}{0pt} % Removes the horizontal line in the headers

\usepackage[backend=biber]{biblatex} % For the bibliography
\addbibresource{refs.bib} % The file containing the bibliography

\usepackage{tcolorbox} % For colored boxes
\usepackage{xcolor} % For colors

\definecolor{blue}{HTML}{66D3F4}
\definecolor{lightBlue}{HTML}{E3F7FD}
\definecolor{yellow}{HTML}{FFDC5C}
\definecolor{lightYellow}{HTML}{FFF9E4}
\definecolor{pink}{HTML}{DE7CC3}
\definecolor{lightPink}{HTML}{F9E7F4}

\newtcolorbox[auto counter]{definition}{colback=lightPink,colframe=pink,fonttitle=\bfseries,fontupper=\itshape,title=Definition~\thetcbcounter}

\newtcolorbox[auto counter]{theorem}{colback=lightBlue,colframe=blue,fonttitle=\bfseries,fontupper=\itshape,title=Theorem~\thetcbcounter}

\newtcolorbox[auto counter]{lemma}{colback=lightYellow,colframe=yellow,fonttitle=\bfseries,fontupper=\itshape,title=Lemma~\thetcbcounter}

\usepackage{hyperref} % For hyperlinks

\usepackage{amsmath} % For mathematical characters
\usepackage{amssymb} % For mathematical symbols
\usepackage{braket} % For Dirac notation

% Removes the indentation of the first line of a paragraph.
\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

\author{Zachary Kokot}
\title{Supplementary Information}

\begin{document}
    \maketitle

    \tableofcontents

    \newpage

    \section{Proof any Density Matrix can be Expressed as a Linear Combination of Pauli Matrices}

    \section{Proof of the Sample Complexity for Complete State Tomography}

    \section{Proof of the Sample Complexity for the Random Local Pauli Measurement Primitive}

    \subsection{Motivation}
    Part of what makes shadow tomography appealing is that it requires a smaller number of measurements to predict an observable within the same error when compared to full state tomography. In this section, we will prove that the random local Pauli measurement primitive requires a number of measurements that scales logarithmically with the number of target observables and exponentially in the locality of the observables. This is a significant improvement over full tomography, which requires a number of measurements that scales exponentially with the number of qubits.

    \subsection{Theorem}

    Fix a collection of $M$ observables, maximum error $\epsilon$, and failure probability $\delta$. Set 
    \begin{equation*}
        K = 2 \log{\left(\frac{2M}{\delta}\right)} \quad \text{and} \quad N \leq \frac{34}{\epsilon^2} \max_{1 \leq i \leq M} 4^{L_i} ||O_i||^2_\infty,
    \end{equation*}

    Where $L_i$ is the locality of the $i$-th observable, $||O||_\infty$ denotes the matrix norm. Then, a collection of $NK$ independent classical shadows allow for the prediction of $M$ observables via median of means prediction such that
    \begin{equation*}
        |\hat{o}_i(N,K) - \text{tr}(O_i\rho)| \leq \epsilon \quad \text{for all} \quad 1 \leq i \leq M
    \end{equation*}
    with probability at least $1-\delta$.

    \subsection{Proof}
    To estimate the expectation value of $M$ observables by median of means estimation with maximum error $\epsilon$ and success probability $1-\delta$ we turn to the results of \cite{Huang_Kueng_Preskill_2020}. In which they state the following theorem:
    \begin{theorem}
        Let X be a random variable with variance $\sigma^2$. Then, K independent sample means of size $N = 34 \frac{\sigma^2}{\epsilon^2}$ suffice to construct a median of means estimator $\hat{\mu}(N, K)$ that obeys 
        \begin{equation}
            \mathbb{P}[|\hat{\mu}(N, K) - \mathbb{E}[X]| \geq \epsilon] \leq 2e^{-K/2}.
        \end{equation}
    \end{theorem}

    They also give the following lemma:
    \begin{lemma}
        Fix an observable $O$ and set $\hat{o} = \text{tr}(O\hat{\rho})$, where $\hat{\rho}$ is a classical shadow. Then
        \begin{equation}
            \text{Var}[\hat{o}] = \mathbb{E}\left[(\hat{o}-\mathbb{E}[\hat{o}^2])\right] \leq \left|\left| O - \frac{\text{tr}(O)}{2^n} \mathbb{I} \right|\right|^2_\text{shadow}.
        \end{equation}
    \end{lemma}

    From the above theorem and lemma, we can conclude that to estimate a single observable the size of each estimator is given by 
    \begin{equation}
        N \leq \frac{34}{\epsilon^2}\left|\left| O - \frac{\text{tr}(O)}{2^n} \mathbb{I} \right|\right|^2_\text{shadow}.
    \end{equation}

    To estimate $M$ observables, we need to take the maximum of the above expression over all observables. This will ensure that for all observables the maximum error is less than $\epsilon$. Thus, the size of each estimator to estimate $M$ observables is given by
    \begin{equation}
        N \leq \frac{34}{\epsilon^2} \max_{1\leq i \leq M} \left|\left| O_i - \frac{\text{tr}(O_i)}{2^n} \mathbb{I} \right|\right|^2_\text{shadow}.
    \end{equation}

    Finally, for the arbitrary Pauli basis measurement primitive we have that the shadow norm has the following property
    \begin{equation}
        \left|\left| O - \frac{\text{tr}(O)}{2^n} \mathbb{I} \right|\right|^2_\text{shadow} \leq 4^{L(O)} ||O||^2_\infty,
    \end{equation}

    Where $L(O)$ is the locality of the observable and $||O||_\infty$ denotes the matrix norm. This implies that the size of each estimator to estimate $M$ observables is given by
    \begin{equation}
        N \leq \frac{34}{\epsilon^2} \max_{1\leq i \leq M} 4^{L_i} ||O_i||^2_\infty.
    \end{equation}

    From the above theorem, we have that for a single random variable the probability the error bound is violated is given by
    \begin{equation}
        \mathbb{P}[|\hat{\mu}(N, K) - \mathbb{E}[X]| \geq \epsilon] \leq 2e^{-K/2}.
    \end{equation}

    We would like to ensure that the probability that the error bound is for any of the $M$ observables is less than $\delta$. To determine an expression for $K$ which satisfies this requirement we can use Booles inequality. Booles inequality states that for any countable set of events $A_1, A_2, \ldots, A_n$ we have
    \begin{equation}
        \mathbb{P}\left[\bigcup_{i=1}^n A_i\right] \leq \sum_{i=1}^n \mathbb{P}[A_i].
    \end{equation}

    In English this means that the probability any event in the set occurs is no greater than the sum of the probabilities of the individual events. Assume the estimation of each observable has a probability of violating the error bound of $\delta_0$. Then, by Booles inequality, the probability that the error bound is violated for any of the $M$ observables is less than
    \begin{equation*}
        \sum_{i=1}^M \mathbb{P}[A_i] = \sum_{i=1}^M \delta_0 = M\delta_0 = \delta.
    \end{equation*}

    Thus, we have that
    \begin{equation*}
        2e^{-K/2} \leq \frac{\delta}{M}.
    \end{equation*}

    Rearranging the above expression we find that
    \begin{equation*}
        K \geq 2\log{\left(\frac{2M}{\delta}\right)}.
    \end{equation*}

    Hence, a collection of $NK$ independent classical shadows allow for the prediction of $M$ observables via median of means prediction such that
    \begin{equation*}
        |\hat{o}_i(N,K) - \text{tr}(O_i\rho)| \leq \epsilon \quad \text{for all} \quad 1 \leq i \leq M
    \end{equation*}
    with probability at least $1-\delta$.
    \hfill $\square$

    \section{Proof of the Computational Complexity for the Estimation of Pauli Observables from shadows generated by the Random Local Pauli Measurement Primitive}

    \subsection{Motivation}
    The computational complexity of the estimation of Pauli observables from shadows generated by the random local Pauli measurement primitive is an important consideration when determining the feasibility of the primitive. In this section, we will prove that the computational complexity of the estimation of Pauli observables from shadows generated by the random local Pauli measurement primitive scales linearly with the number of qubits, the number of target observables, the size of the shadow.

    \subsection{Theorem}
    Fix a collection of $M$ Pauli observables, a collection of $NK$ independent classical shadows, and a system size (number of qubits) $n$. Then, the computational complexity of the estimation of $M$ observables from the shadows scales with
    \begin{equation*}
        \mathcal{O}\left(MNKn\right).
    \end{equation*}

    \subsection{Proof}
    The estimation of Pauli observables using a collection of $NK$ independent classical shadows generated by the random local Pauli measurement primitive can be broken down into the following steps for a single observable:
    \begin{enumerate}
        \item Split the $NK$ independent classical shadows into $K$ groups of size $N$.
        \item Iterate over each group of shadows and estimate the expectation value of the Pauli observable.
        \begin{enumerate}
            \item Iterate over each shadow in the group.
            \item For each shadow in the group, obtain the single shot expectation value for the Pauli observable.
            \item Compute the mean of the single shot expectation values.
        \end{enumerate}
        \item Compute the median of the means of the estimates of the Pauli observable.
    \end{enumerate}

    It is clear to see how the terms $N$ and $K$ arise as the estimation protocol simply loops over all $NK$ terms in the collection of shadows.

    The term $n$ arises from the way in which each single shot expectation value is obtained. See Section \ref{sec:expectation} for more details.

    Finally, the term $M$ arises from the fact that the estimation protocol must be repeated for each of the $M$ Pauli observables. This is because the estimation of each Pauli observable not dependent of the others in general.

    \section{Computing the Expectation for a Single Pauli Observable} \label{sec:expectation}
    \subsection{Motivation}
    In this section, we will outline how to compute the expectation value of a single Pauli observable using the random local Pauli measurement primitive. This implementation avoids the need for reconstructing a single shot density matrix representation of the state and the computation of a possibly very large matrix product. This offers the potential for a computational speedup by avoiding more expensive operations. Additionally, this implementation does not require the exponentially growing density matrix to be stored in memory.

    \subsection{Method}
    The expectation value of a single Pauli observable can be computed from a collection of shadows using the following equation. Let $O$ be a Pauli observable and $\hat{\rho}$ be a classical shadow. Then, the single shot expectation value of the Pauli observable is given by:
    \begin{equation}
        \hat{o} = \prod_{j=1}^n \left[ \text{tr}\{ 3 U_jP_j U^\dagger_j \ket{\hat{b}_j}\bra{\hat{b}_j}\} - \text{tr}\{ P_j \} \right].
    \end{equation}

    Where $P_j$ is the $j$-th Pauli operator, $U_j$ is the $j$-th unitary operator, and $\ket{\hat{b}_j}$ is the $j$-th bit of the bitstring of the shadow. Where the above expression evaluates to $\pm3^{\text{Locality}(O)}$ if for each $j$, $U_j$ transforms $P_j$ to $Z$ (the Pauli-Z operator). Otherwise the expression evaluates to 0. Then the expectation value of the Pauli observable is given by the mean of the single shot expectation values.
    
    \subsection{Proof}


    \printbibliography[title={References}]

\end{document}